{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Age of Empires II Voobly community statistics__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load libraries and set options/environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyodbc\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from functools import reduce\n",
    "import datetime\n",
    "from datetime import date, timedelta\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import mechanize\n",
    "import http.cookiejar\n",
    "import requests\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up ODBC connection and SQL engine for write back\n",
    "def setup_sql_conn():\n",
    "    \n",
    "    driver = '{SQL Server Native Client 11.0}'\n",
    "    server = 'localhost'\n",
    "    database = 'AOE2_VOOBLY'\n",
    "    port = 1433\n",
    "\n",
    "    db_conn = pyodbc.connect('DRIVER='+driver+\n",
    "                             ';PORT='+str(port)+\n",
    "                             ';SERVER='+server+\n",
    "                             ';DATABASE='+database+\n",
    "                             ';Trusted_Connection=yes')\n",
    "\n",
    "    db_cursor = db_conn.cursor()\n",
    "\n",
    "    engine = create_engine(\"mssql+pyodbc://localhost/AOE2_VOOBLY?driver=SQL+Server+Native+Client+11.0\")\n",
    "    return(db_conn,db_cursor,engine)\n",
    "\n",
    "# fetch max match ID in DB\n",
    "def fetch_latest_match_id(db_conn):\n",
    "    \n",
    "    data = pd.DataFrame(pd.read_sql_query('''SELECT MAX(CAST(A.Match_ID AS INT)) FROM [AOE2_VOOBLY].[dbo].[RAW_MATCH_DATA] A''', db_conn))\n",
    "    data.columns = ['head']\n",
    "    return(data['head'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define functions \n",
    "1. **Login** - login to the main page using existing credentials\n",
    "2. **Pick lobbies & ladders to scrape** - Focus would be on New player and RM/DM lobbies & RM and DM 1x1 ladders which are the most competitive ranked ladders\n",
    "3. **Iterate over players and fetch match details** - Match details to include post match economy, military stats along with win/loss records and civilizations picked. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Login:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login into voobly\n",
    "def voobly_login(username, password, proxy = None, port = None):\n",
    "    # Browser\n",
    "    br = mechanize.Browser()\n",
    "\n",
    "    # Cookie Jar\n",
    "    cj = http.cookiejar.LWPCookieJar()\n",
    "    br.set_cookiejar(cj)\n",
    "\n",
    "    # Browser options\n",
    "    if (not proxy) and (not port):\n",
    "        br.set_proxies({\"https\":proxy+\":\"+port})\n",
    "    br.set_handle_equiv(True)\n",
    "    br.set_handle_gzip(True)\n",
    "    br.set_handle_redirect(True)\n",
    "    br.set_handle_referer(True)\n",
    "    br.set_handle_robots(False)\n",
    "    br.set_handle_refresh(mechanize._http.HTTPRefreshProcessor(), max_time=1)\n",
    "\n",
    "    br.addheaders = [('User-agent', 'Chrome')]\n",
    "\n",
    "    # The site we will navigate into, handling it's session\n",
    "    br.open('https://voobly.com/login')\n",
    "\n",
    "    # Select the second (index one) form (the first form is a search query box)\n",
    "    br.select_form(nr=0)\n",
    "\n",
    "    # User credentials\n",
    "    br.form['username'] = username\n",
    "    br.form['password'] = password\n",
    "\n",
    "    # Login\n",
    "    br.submit()\n",
    "    \n",
    "    return(br)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Pick lobbies & ladders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all lobbies and ladders\n",
    "def fetch_lobby_ladders(instance):\n",
    "    \n",
    "    lobby_page = 'https://www.voobly.com/games/view/Age-of-Empires-II-The-Conquerors'\n",
    "    soup = BeautifulSoup(instance.open(lobby_page).read())\n",
    "    table = soup.find('table').find_all('td')\n",
    "\n",
    "    # Create dataframe of links\n",
    "    data = pd.DataFrame(columns=['Lobby', 'Lobby_link', 'Ladder', 'Ladder_link'])\n",
    "    for i in table:\n",
    "        lobby = i.find_all('a')\n",
    "        if len(lobby) == 0:\n",
    "            continue \n",
    "        for j in lobby:\n",
    "            if 'games' in j['href']:\n",
    "                lobby_text = j.text\n",
    "                lobby_link = j['href']        \n",
    "            if lobby_text == j.text:\n",
    "                continue    \n",
    "            ladder_text = j.text\n",
    "            ladder_link = j['href']        \n",
    "\n",
    "            # fill dataframe\n",
    "            row = pd.DataFrame.from_dict({'Lobby':[lobby_text], 'Lobby_link':[lobby_link], 'Ladder':[ladder_text], 'Ladder_link':[ladder_link]})\n",
    "            data = data.append(row)\n",
    "    data = data.set_index([pd.Index(range(0,data.shape[0],1))])\n",
    "    \n",
    "    return(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Fetch match details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch soup and tags:\n",
    "def fetch_html(match_id,instance):\n",
    "    \n",
    "    match_page = 'https://www.voobly.com/match/view/'+str(match_id)+'/Match-Details'    \n",
    "    \n",
    "    try:\n",
    "        obj = instance.open(match_page,timeout = 10)\n",
    "        soup = BeautifulSoup(obj.read())\n",
    "        table = soup.find_all('table')    \n",
    "    \n",
    "        all_tags = []\n",
    "        if len(table)>0:\n",
    "\n",
    "            for i in table:\n",
    "                all_tags.append(i.text)\n",
    "            all_tags = [re.sub('\\n+', '||', sub) for sub in all_tags]\n",
    "            all_tags = reduce(lambda l, x: l.append(x) or l if x not in l else l, all_tags, [])            \n",
    "    \n",
    "        return(soup,all_tags,1)\n",
    "    \n",
    "    except:\n",
    "        return([],[],0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch match ladder\n",
    "def fetch_match_ladder(all_tags):   \n",
    "    \n",
    "    title_tags = all_tags[0].split(sep=\"||\")\n",
    "    title_tags = [x for x in title_tags if x]\n",
    "    ladder = title_tags[1]\n",
    "    \n",
    "    return(ladder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch match details\n",
    "def fetch_match_details(all_tags):   \n",
    "    \n",
    "    detail_tags = all_tags[2].split(sep=\"||\")\n",
    "    detail_tags = pd.DataFrame([x for x in detail_tags if x])\n",
    "    detail_tags.columns = ['head']\n",
    "\n",
    "    match_id = int(re.sub('#','',detail_tags['head'][int(detail_tags[detail_tags['head']=='Match Details'].index[0])+1]))\n",
    "    \n",
    "    date_time = detail_tags['head'][int(detail_tags[detail_tags['head']=='Date Played:'].index[0])+1]\n",
    "    if 'Yesterday' in  date_time:\n",
    "        match_date = (date.today() - timedelta(days=1)).strftime('%m/%d/%Y')\n",
    "        match_time = datetime.datetime.strptime(date_time.split(', ')[1],'%I:%M %p').strftime('%H:%M')\n",
    "    elif 'Today' in  date_time:\n",
    "        match_date = date.today().strftime('%m/%d/%Y')\n",
    "        match_time = datetime.datetime.strptime(date_time.split(', ')[1],'%I:%M %p').strftime('%H:%M')\n",
    "    else:\n",
    "        match_date = datetime.datetime.strptime(date_time.split(' - ')[0],'%d %B %Y').strftime('%m/%d/%Y')\n",
    "        match_time = datetime.datetime.strptime(date_time.split(' - ')[1],'%I:%M %p').strftime('%H:%M')\n",
    "    \n",
    "    match_rating = int(detail_tags['head'][int(detail_tags[detail_tags['head']=='Match Rating:'].index[0])+1])\n",
    "    match_map = detail_tags['head'][int(detail_tags[detail_tags['head']=='Map:'].index[0])+1]\n",
    "    match_length = detail_tags['head'][int(detail_tags[detail_tags['head']=='Duration:'].index[0])+1]\n",
    "    match_player_no = int(detail_tags['head'][int(detail_tags[detail_tags['head']=='Players:'].index[0])+1])\n",
    "    match_mod = detail_tags['head'][int(detail_tags[detail_tags['head']=='Game Mod:'].index[0])+1]\n",
    "\n",
    "    match_details = {'Match_ID':[match_id],'Match_date':[match_date],'Match_time':[match_time],'Match_rating':[match_rating],\n",
    "                     'Match_map':[match_map],'Match_length':[match_length],'Match_player_no':[match_player_no],'Match_mod':[match_mod]}\n",
    "\n",
    "    match_details = pd.DataFrame.from_dict(match_details)\n",
    "    return(match_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch player details\n",
    "def fetch_player_details(soup,all_tags):    \n",
    "    \n",
    "    player_tags = all_tags[5].split(sep=\"||\")\n",
    "    player_tags = pd.DataFrame([x for x in player_tags if x and not str(x).isspace()])\n",
    "    player_tags.columns = ['head']\n",
    "    player_tags = pd.concat([pd.DataFrame(player_tags['head'][[num for num in range(player_tags.shape[0]) if num % 2 == 0]]).reset_index(),\n",
    "                             pd.DataFrame(player_tags['head'][[num for num in range(player_tags.shape[0]) if num % 2 != 0]]).reset_index()],axis=1).drop(['index'],axis=1)\n",
    "    player_tags.columns = ['Player','Details']\n",
    "\n",
    "    player_c_tags = player_tags[player_tags.Player.str.contains('\\(Computer\\)$')]\n",
    "    player_tags = player_tags[~player_tags.Player.str.contains('\\(Computer\\)$')]\n",
    "\n",
    "    if player_c_tags.shape[0]>0:    \n",
    "        player_c_tags['New Rating'] = ''\n",
    "        player_c_tags['Points'] = ''\n",
    "        player_c_tags['Team'] = player_c_tags.Details.apply(lambda x: int(x.split('Team: ')[1].split(' ')[0]))\n",
    "        player_c_tags = player_c_tags.drop(['Details'],axis=1)\n",
    "\n",
    "    player_tags['New Rating'] = player_tags.Details.apply(lambda x: int(x.split('New Rating: ')[1].split(' ')[0]))\n",
    "    player_tags['Points'] = player_tags.Details.apply(lambda x: int(x.split('Points: ')[1].split(' ')[0]))\n",
    "    player_tags['Team'] = player_tags.Details.apply(lambda x: int(x.split('Team: ')[1].split(' ')[0]))\n",
    "    player_tags = player_tags.drop(['Details'],axis=1)\n",
    "    \n",
    "    if player_c_tags.shape[0]>0:\n",
    "        player_tags = pd.concat([player_tags,player_c_tags],axis=0)\n",
    "\n",
    "    civs = [re.search('(^\\\\|\\\\|[A-Z][A-Z][A-Z]\\\\|\\\\||)',x).group(1) for x in all_tags]\n",
    "    civs = [x for x in civs if x]\n",
    "    civs = pd.DataFrame([re.sub(\"\\|\\|\",\"\",x) for x in civs])\n",
    "    player_tags['Civilization'] = civs\n",
    "    player_no = player_tags.shape[0]\n",
    "    \n",
    "    images = soup.find_all('img')\n",
    "    win_players = []\n",
    "    for i in range(len(images)):\n",
    "        if re.search('win.PNG', str(images[i])):\n",
    "            string = str(images[i-1])\n",
    "            start = [x.start() for x in re.finditer('\\\"', string)][0]+1\n",
    "            end = [x.start() for x in re.finditer('\\\"', string)][1]        \n",
    "            win_players.append(string[start:end])\n",
    "\n",
    "    player_tags['Victory'] = player_tags.Player.apply(lambda x: sum([i in x for i in win_players]))    \n",
    "    \n",
    "    return(player_tags,player_no)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create scores metadata table\n",
    "def create_metadata_table():\n",
    "    \n",
    "    score_table = [0, 5, 5, 7, 6, 5]\n",
    "    score_table = pd.DataFrame(score_table, columns = ['Columns']) \n",
    "    score_table = pd.concat([score_table,score_table.cumsum()],axis=1)\n",
    "    score_table.columns = ['Columns','CumColumns']\n",
    "    \n",
    "    return(score_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch match scores for all players\n",
    "def fetch_match_scores(soup,score_table,player_no):\n",
    "    \n",
    "    table = soup.find_all('center')\n",
    "\n",
    "    score_tags = []\n",
    "    for i in table:\n",
    "        score_tags.append(i.text)\n",
    "        \n",
    "    start_index = [ i for i, word in enumerate(score_tags) if re.search('Military Score', word)]    \n",
    "    del score_tags[0:int(start_index[0])]\n",
    "\n",
    "    for k in range(score_table.shape[0]-1):\n",
    "\n",
    "        score_tag_headers = score_tags[(player_no*score_table.CumColumns[k])+score_table.CumColumns[k]:(player_no*score_table.CumColumns[k])+score_table.CumColumns[k+1]]\n",
    "        score_tag = pd.DataFrame(score_tags[(player_no*score_table.CumColumns[k])+score_table.CumColumns[k+1]:(player_no*score_table.CumColumns[k+1])+score_table.CumColumns[k+1]])\n",
    "        score_tag.columns = ['head']\n",
    "\n",
    "        indicies = [num for num in range(score_tag.shape[0]) if num % score_table.Columns[k+1] == 0]\n",
    "\n",
    "        for i in range(score_table.Columns[k+1]):\n",
    "            c_index = [x+i for x in indicies]\n",
    "            col = pd.DataFrame(score_tag['head'][c_index]).reset_index().drop(['index'],axis=1)\n",
    "            col.columns = [score_tag_headers[i]]\n",
    "            if i==0:\n",
    "                score = col\n",
    "            else:\n",
    "                score = pd.concat([score,col],axis = 1)\n",
    "\n",
    "        if k == 0:\n",
    "            match_score = score\n",
    "        else:\n",
    "            match_score = pd.concat([match_score,score],axis = 1)\n",
    "            \n",
    "    return(match_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_details(ladder,match_details,player_details,match_scores,player_no):\n",
    "    \n",
    "    match_data = pd.DataFrame(pd.Series([ladder]).repeat(player_no))\n",
    "    match_data.columns = ['ladder']\n",
    "    match_data = pd.concat([match_data,pd.concat([match_details]*player_no)],axis=1)\n",
    "    match_data = match_data.set_index([pd.Index(range(0,match_data.shape[0],1))])\n",
    "    match_data = pd.concat([match_data,player_details,match_scores],axis = 1)\n",
    "    return(match_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_all_match_details(match_id,instance):\n",
    "    \n",
    "    # Fetch html data\n",
    "    soup,all_tags,status = fetch_html(match_id,instance)\n",
    "    \n",
    "    if status == 1:        \n",
    "    \n",
    "        if len(all_tags) !=0:\n",
    "            \n",
    "            try:\n",
    "                # Match ladder\n",
    "                ladder = fetch_match_ladder(all_tags)\n",
    "\n",
    "                # Match details\n",
    "                match_details = fetch_match_details(all_tags)\n",
    "\n",
    "                # Player details\n",
    "                player_details,player_no = fetch_player_details(soup,all_tags)\n",
    "\n",
    "                # Match scores\n",
    "                score_table = create_metadata_table()\n",
    "                match_scores = fetch_match_scores(soup,score_table,player_no)\n",
    "\n",
    "                match_data = aggregate_details(ladder,match_details,player_details,match_scores,player_no)\n",
    "\n",
    "                for column in match_data:\n",
    "                    match_data[column] = match_data[column].astype(str)\n",
    "                match_data.columns = match_data.columns.str.replace(' ', '_')\n",
    "\n",
    "                return(match_data)\n",
    "            except:\n",
    "                return([3]) # Not an AOE2 Match\n",
    "        else:\n",
    "            return([2]) # Match ID does not exist  \n",
    "    else:\n",
    "        return([1]) # Connection timed out  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to iterate over multiple matches\n",
    "def fetch_matches_iterate(start,collect,instance,engine):\n",
    "    \n",
    "    fails = []\n",
    "    timeout_fails = []\n",
    "    timeout_counter = 0\n",
    "\n",
    "    for index in tqdm(range(collect),miniters = 1,bar_format='{desc:<5.5}{percentage:3.0f}%|{bar:60}{r_bar}'):\n",
    "        \n",
    "        if timeout_counter == 10:\n",
    "            \n",
    "            print(\"\\nABORTING: 10 consecutive timeout failures, check your proxy/connections settings !\")\n",
    "            break\n",
    "        else:\n",
    "            \n",
    "            obj = pd.Series.to_frame(pd.Series([start], dtype='str'))\n",
    "            obj.columns = ['Match_ID']       \n",
    "\n",
    "            try:            \n",
    "                data = fetch_all_match_details(start,instance)\n",
    "\n",
    "                if len(data) == 1 and data == [2]:\n",
    "                    obj.to_sql('NONEXISTENT_FAILS',con=engine, if_exists=\"append\",index=False)\n",
    "                    fails.append(start)\n",
    "\n",
    "                elif len(data) == 1 and data == [1]:\n",
    "                    obj.to_sql('TIMEOUT_FAILS',con=engine, if_exists=\"append\",index=False)\n",
    "                    timeout_fails.append(start)\n",
    "\n",
    "                    if len(timeout_fails) != 0:\n",
    "                        if max(timeout_fails) == previous_start:\n",
    "                            timeout_counter+=1\n",
    "\n",
    "                elif len(data) == 1 and data == [3]:\n",
    "                    obj.to_sql('NONAOE2MATCH_FAILS',con=engine, if_exists=\"append\",index=False)\n",
    "                    fails.append(start)\n",
    "\n",
    "                else:\n",
    "                    data.to_sql('RAW_MATCH_DATA',con=engine, if_exists=\"append\",index=False)\n",
    "            except:\n",
    "                obj.to_sql('UNKNOWN_FAILS',con=engine, if_exists=\"append\",index=False)\n",
    "                fails.append(start)\n",
    "\n",
    "            previous_start = start    \n",
    "            start+=1\n",
    "    \n",
    "    print('\\nDONE !')\n",
    "    print('\\nAOE2 MATCHES DOWNLOADED : ' + str(collect - len(fails)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
