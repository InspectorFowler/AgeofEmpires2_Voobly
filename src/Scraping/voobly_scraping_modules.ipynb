{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Age of Empires II Voobly community scraping modules__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Define functions__\n",
    "1. **Proxy** - Setup proxy settings by randomly picking a proxy/port from online list\n",
    "2. **Database** - Setup database connection strings and variables. Might have to change based on OS (Linux/Win)\n",
    "3. **Login** - login to the main page using existing credentials\n",
    "4. **Pick lobbies & ladders to scrape** - Focus would be on New player and RM/DM lobbies & RM and DM 1x1 ladders which are the most competitive ranked ladders\n",
    "5. **Fetch match details** - Match details to include post match economy, military stats along with win/loss records and civilizations picked. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __1. Proxy Setup :__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_proxies():   \n",
    "\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.77 Safari/537.36'}\n",
    "    URL = \"https://free-proxy-list.net/\"\n",
    "    req = requests.get(URL, headers = headers) # .json()\n",
    "    soup = BeautifulSoup(req.text, \"lxml\")\n",
    "\n",
    "    for body in soup(\"tbody\"):\n",
    "        body.unwrap()\n",
    "\n",
    "    df = pd.read_html(str(soup), flavor=\"bs4\")\n",
    "    df = pd.DataFrame(df[0])\n",
    "    proxies = df[(df.Https == 'yes')] # (df.Https == 'yes') & (df.Country == 'United States')\n",
    "    proxies['Port'] = proxies['Port'].astype(int)\n",
    "    proxies = proxies[['IP Address','Port']][(proxies['Https']=='yes') & (proxies['Anonymity']=='elite proxy')]\n",
    "    \n",
    "    index = random.randint(0,proxies.shape[0])\n",
    "    \n",
    "    return(str(proxies.iloc[index,]['IP Address']),str(proxies.iloc[index,]['Port']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __2. Database connection :__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up ODBC connection and SQL engine for write back\n",
    "def setup_sql_conn():\n",
    "    \n",
    "    server = 'localhost'\n",
    "    database = 'AOE2_VOOBLY'\n",
    "    user = 'admin'\n",
    "    password = 'admin'\n",
    "\n",
    "    db_conn = mysql.connector.connect(host = server,\n",
    "                                      port = 3306,\n",
    "                                      user = user,\n",
    "                                      password = password)\n",
    "\n",
    "    engine = create_engine(\"mysql+mysqlconnector://\" + user + \":\" + password + \"@\" + server + \"/\" + database)\n",
    "    return(db_conn,engine)\n",
    "\n",
    "# fetch max match ID in DB\n",
    "def fetch_latest_match_id(db_conn):\n",
    "    \n",
    "    data = pd.DataFrame(pd.read_sql_query('''SELECT MAX(A.Match_ID) FROM AOE2_VOOBLY.RAW_MATCH_DATA A''', db_conn))\n",
    "    data.columns = ['head']\n",
    "    return(int(data['head'][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __3. Voobly Login :__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login into voobly\n",
    "def voobly_login(username, password, proxy = None, port = None):\n",
    "    # Browser\n",
    "    br = mechanize.Browser()\n",
    "\n",
    "    # Cookie Jar\n",
    "    cj = http.cookiejar.LWPCookieJar()\n",
    "    br.set_cookiejar(cj)\n",
    "\n",
    "    # Browser options\n",
    "    if (not proxy) and (not port):\n",
    "        br.set_proxies({\"https\":proxy+\":\"+port})\n",
    "    br.set_handle_equiv(True)\n",
    "    br.set_handle_gzip(True)\n",
    "    br.set_handle_redirect(True)\n",
    "    br.set_handle_referer(True)\n",
    "    br.set_handle_robots(False)\n",
    "    br.set_handle_refresh(mechanize._http.HTTPRefreshProcessor(), max_time=1)\n",
    "\n",
    "    br.addheaders = [('User-agent', 'Chrome')]\n",
    "\n",
    "    # The site we will navigate into, handling it's session\n",
    "    br.open('https://voobly.com/login')\n",
    "\n",
    "    # Select the second (index one) form (the first form is a search query box)\n",
    "    br.select_form(nr=0)\n",
    "\n",
    "    # User credentials\n",
    "    br.form['username'] = username\n",
    "    br.form['password'] = password\n",
    "\n",
    "    # Login\n",
    "    br.submit()\n",
    "    \n",
    "    return(br)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __4. Pick lobbies and ladders :__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all lobbies and ladders\n",
    "def fetch_lobby_ladders(instance):\n",
    "    \n",
    "    lobby_page = 'https://www.voobly.com/games/view/Age-of-Empires-II-The-Conquerors'\n",
    "    soup = BeautifulSoup(instance.open(lobby_page).read())\n",
    "    table = soup.find('table').find_all('td')\n",
    "\n",
    "    # Create dataframe of links\n",
    "    data = pd.DataFrame(columns=['Lobby', 'Lobby_link', 'Ladder', 'Ladder_link'])\n",
    "    for i in table:\n",
    "        lobby = i.find_all('a')\n",
    "        if len(lobby) == 0:\n",
    "            continue \n",
    "        for j in lobby:\n",
    "            if 'games' in j['href']:\n",
    "                lobby_text = j.text\n",
    "                lobby_link = j['href']        \n",
    "            if lobby_text == j.text:\n",
    "                continue    \n",
    "            ladder_text = j.text\n",
    "            ladder_link = j['href']        \n",
    "\n",
    "            # fill dataframe\n",
    "            row = pd.DataFrame.from_dict({'Lobby':[lobby_text], 'Lobby_link':[lobby_link], 'Ladder':[ladder_text], 'Ladder_link':[ladder_link]})\n",
    "            data = data.append(row)\n",
    "    data = data.set_index([pd.Index(range(0,data.shape[0],1))])\n",
    "    \n",
    "    return(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __5. Fetch match details :__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch soup and tags:\n",
    "def fetch_html(match_id,instance):\n",
    "    \n",
    "    match_page = 'https://www.voobly.com/match/view/'+str(match_id)+'/Match-Details'    \n",
    "    \n",
    "    try:\n",
    "        obj = instance.open(match_page,timeout = 10)\n",
    "        soup = BeautifulSoup(obj.read())\n",
    "        table = soup.find_all('table')    \n",
    "    \n",
    "        all_tags = []\n",
    "        if len(table)>0:\n",
    "\n",
    "            for i in table:\n",
    "                all_tags.append(i.text)\n",
    "            all_tags = [re.sub('\\n+', '||', sub) for sub in all_tags]\n",
    "            all_tags = reduce(lambda l, x: l.append(x) or l if x not in l else l, all_tags, [])            \n",
    "    \n",
    "        return(soup,all_tags,1)\n",
    "    \n",
    "    except:\n",
    "        return([],[],0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch match ladder\n",
    "def fetch_match_ladder(all_tags):   \n",
    "    \n",
    "    title_tags = all_tags[0].split(sep=\"||\")\n",
    "    title_tags = [x for x in title_tags if x]\n",
    "    ladder = title_tags[1]\n",
    "    \n",
    "    return(ladder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch match details\n",
    "def fetch_match_details(all_tags):   \n",
    "    \n",
    "    detail_tags = all_tags[2].split(sep=\"||\")\n",
    "    detail_tags = pd.DataFrame([x for x in detail_tags if x])\n",
    "    detail_tags.columns = ['head']\n",
    "\n",
    "    match_id = int(re.sub('#','',detail_tags['head'][int(detail_tags[detail_tags['head']=='Match Details'].index[0])+1]))\n",
    "    \n",
    "    date_time = detail_tags['head'][int(detail_tags[detail_tags['head']=='Date Played:'].index[0])+1]\n",
    "    if 'Yesterday' in  date_time:\n",
    "        match_date = (date.today() - timedelta(days=1)).strftime('%m/%d/%Y')\n",
    "        match_time = datetime.datetime.strptime(date_time.split(', ')[1],'%I:%M %p').strftime('%H:%M')\n",
    "    elif 'Today' in  date_time:\n",
    "        match_date = date.today().strftime('%m/%d/%Y')\n",
    "        match_time = datetime.datetime.strptime(date_time.split(', ')[1],'%I:%M %p').strftime('%H:%M')\n",
    "    else:\n",
    "        match_date = datetime.datetime.strptime(date_time.split(' - ')[0],'%d %B %Y').strftime('%m/%d/%Y')\n",
    "        match_time = datetime.datetime.strptime(date_time.split(' - ')[1],'%I:%M %p').strftime('%H:%M')\n",
    "    \n",
    "    match_rating = int(detail_tags['head'][int(detail_tags[detail_tags['head']=='Match Rating:'].index[0])+1])\n",
    "    match_map = detail_tags['head'][int(detail_tags[detail_tags['head']=='Map:'].index[0])+1]\n",
    "    match_length = detail_tags['head'][int(detail_tags[detail_tags['head']=='Duration:'].index[0])+1]\n",
    "    match_player_no = int(detail_tags['head'][int(detail_tags[detail_tags['head']=='Players:'].index[0])+1])\n",
    "    match_mod = detail_tags['head'][int(detail_tags[detail_tags['head']=='Game Mod:'].index[0])+1]\n",
    "\n",
    "    match_details = {'Match_ID':[match_id],'Match_date':[match_date],'Match_time':[match_time],'Match_rating':[match_rating],\n",
    "                     'Match_map':[match_map],'Match_length':[match_length],'Match_player_no':[match_player_no],'Match_mod':[match_mod]}\n",
    "\n",
    "    match_details = pd.DataFrame.from_dict(match_details)\n",
    "    return(match_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch player details\n",
    "def fetch_player_details(soup,all_tags):    \n",
    "    \n",
    "    player_tags = all_tags[5].split(sep=\"||\")\n",
    "    player_tags = pd.DataFrame([x for x in player_tags if x and not str(x).isspace()])\n",
    "    player_tags.columns = ['head']\n",
    "    player_tags = pd.concat([pd.DataFrame(player_tags['head'][[num for num in range(player_tags.shape[0]) if num % 2 == 0]]).reset_index(),\n",
    "                             pd.DataFrame(player_tags['head'][[num for num in range(player_tags.shape[0]) if num % 2 != 0]]).reset_index()],axis=1).drop(['index'],axis=1)\n",
    "    player_tags.columns = ['Player','Details']\n",
    "\n",
    "    player_c_tags = player_tags[player_tags.Player.str.contains('\\(Computer\\)$')]\n",
    "    player_tags = player_tags[~player_tags.Player.str.contains('\\(Computer\\)$')]\n",
    "\n",
    "    if player_c_tags.shape[0]>0:    \n",
    "        player_c_tags['New Rating'] = ''\n",
    "        player_c_tags['Points'] = ''\n",
    "        player_c_tags['Team'] = player_c_tags.Details.apply(lambda x: int(x.split('Team: ')[1].split(' ')[0]))\n",
    "        player_c_tags = player_c_tags.drop(['Details'],axis=1)\n",
    "\n",
    "    player_tags['New Rating'] = player_tags.Details.apply(lambda x: int(x.split('New Rating: ')[1].split(' ')[0]))\n",
    "    player_tags['Points'] = player_tags.Details.apply(lambda x: int(x.split('Points: ')[1].split(' ')[0]))\n",
    "    player_tags['Team'] = player_tags.Details.apply(lambda x: int(x.split('Team: ')[1].split(' ')[0]))\n",
    "    player_tags = player_tags.drop(['Details'],axis=1)\n",
    "    \n",
    "    if player_c_tags.shape[0]>0:\n",
    "        player_tags = pd.concat([player_tags,player_c_tags],axis=0)\n",
    "\n",
    "    civs = [re.search('(^\\\\|\\\\|[A-Z][A-Z][A-Z]\\\\|\\\\||)',x).group(1) for x in all_tags]\n",
    "    civs = [x for x in civs if x]\n",
    "    civs = pd.DataFrame([re.sub(\"\\|\\|\",\"\",x) for x in civs])\n",
    "    player_tags['Civilization'] = civs\n",
    "    player_no = player_tags.shape[0]\n",
    "    \n",
    "    images = soup.find_all('img')\n",
    "    win_players = []\n",
    "    for i in range(len(images)):\n",
    "        if re.search('win.PNG', str(images[i])):\n",
    "            string = str(images[i-1])\n",
    "            start = [x.start() for x in re.finditer('\\\"', string)][0]+1\n",
    "            end = [x.start() for x in re.finditer('\\\"', string)][1]        \n",
    "            win_players.append(string[start:end])\n",
    "\n",
    "    player_tags['Victory'] = player_tags.Player.apply(lambda x: sum([i in x for i in win_players]))    \n",
    "    \n",
    "    return(player_tags,player_no)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create scores metadata table\n",
    "def create_metadata_table():\n",
    "    \n",
    "    score_table = [0, 5, 5, 7, 6, 5]\n",
    "    score_table = pd.DataFrame(score_table, columns = ['Columns']) \n",
    "    score_table = pd.concat([score_table,score_table.cumsum()],axis=1)\n",
    "    score_table.columns = ['Columns','CumColumns']\n",
    "    \n",
    "    return(score_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch match scores for all players\n",
    "def fetch_match_scores(soup,score_table,player_no):\n",
    "    \n",
    "    table = soup.find_all('center')\n",
    "\n",
    "    score_tags = []\n",
    "    for i in table:\n",
    "        score_tags.append(i.text)\n",
    "        \n",
    "    start_index = [ i for i, word in enumerate(score_tags) if re.search('Military Score', word)]    \n",
    "    del score_tags[0:int(start_index[0])]\n",
    "\n",
    "    for k in range(score_table.shape[0]-1):\n",
    "\n",
    "        score_tag_headers = score_tags[(player_no*score_table.CumColumns[k])+score_table.CumColumns[k]:(player_no*score_table.CumColumns[k])+score_table.CumColumns[k+1]]\n",
    "        score_tag = pd.DataFrame(score_tags[(player_no*score_table.CumColumns[k])+score_table.CumColumns[k+1]:(player_no*score_table.CumColumns[k+1])+score_table.CumColumns[k+1]])\n",
    "        score_tag.columns = ['head']\n",
    "\n",
    "        indicies = [num for num in range(score_tag.shape[0]) if num % score_table.Columns[k+1] == 0]\n",
    "\n",
    "        for i in range(score_table.Columns[k+1]):\n",
    "            c_index = [x+i for x in indicies]\n",
    "            col = pd.DataFrame(score_tag['head'][c_index]).reset_index().drop(['index'],axis=1)\n",
    "            col.columns = [score_tag_headers[i]]\n",
    "            if i==0:\n",
    "                score = col\n",
    "            else:\n",
    "                score = pd.concat([score,col],axis = 1)\n",
    "\n",
    "        if k == 0:\n",
    "            match_score = score\n",
    "        else:\n",
    "            match_score = pd.concat([match_score,score],axis = 1)\n",
    "            \n",
    "    return(match_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_details(ladder,match_details,player_details,match_scores,player_no):\n",
    "    \n",
    "    match_data = pd.DataFrame(pd.Series([ladder]).repeat(player_no))\n",
    "    match_data.columns = ['ladder']\n",
    "    match_data = pd.concat([match_data,pd.concat([match_details]*player_no)],axis=1)\n",
    "    match_data = match_data.set_index([pd.Index(range(0,match_data.shape[0],1))])\n",
    "    match_data = pd.concat([match_data,player_details,match_scores],axis = 1)\n",
    "    \n",
    "    return(match_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_all_match_details(match_id,instance):\n",
    "    \n",
    "    # Fetch html data\n",
    "    soup,all_tags,status = fetch_html(match_id,instance)\n",
    "    \n",
    "    if status == 1:        \n",
    "    \n",
    "        if len(all_tags) !=0:\n",
    "            \n",
    "            try:\n",
    "                # Match ladder\n",
    "                ladder = fetch_match_ladder(all_tags)\n",
    "\n",
    "                # Match details\n",
    "                match_details = fetch_match_details(all_tags)\n",
    "\n",
    "                # Player details\n",
    "                player_details,player_no = fetch_player_details(soup,all_tags)\n",
    "\n",
    "                # Match scores\n",
    "                score_table = create_metadata_table()\n",
    "                match_scores = fetch_match_scores(soup,score_table,player_no)\n",
    "\n",
    "                match_data = aggregate_details(ladder,match_details,player_details,match_scores,player_no)\n",
    "\n",
    "                for column in match_data:\n",
    "                    match_data[column] = match_data[column].astype(str)\n",
    "                match_data.columns = match_data.columns.str.replace(' ', '_')\n",
    "\n",
    "                return(match_data)\n",
    "            except:\n",
    "                return([3]) # Not an AOE2 Match\n",
    "        else:\n",
    "            return([2]) # Match ID does not exist  \n",
    "    else:\n",
    "        return([1]) # Connection timed out  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch match details and write to DB\n",
    "def fetch_matches_write(match_id,instance,engine):    \n",
    "    \n",
    "    obj = pd.Series.to_frame(pd.Series([match_id], dtype='str'))\n",
    "    obj.columns = ['Match_ID']   \n",
    "        \n",
    "    try:            \n",
    "        data = fetch_all_match_details(match_id,instance)\n",
    "\n",
    "        if len(data) == 1 and data == [2]:\n",
    "            obj.to_sql('NONEXISTENT_FAILS',con=engine, if_exists=\"append\",index=False)\n",
    "            fails.append(match_id)\n",
    "\n",
    "        elif len(data) == 1 and data == [1]:\n",
    "            obj.to_sql('TIMEOUT_FAILS',con=engine, if_exists=\"append\",index=False)\n",
    "            timeout_fails.append(match_id)\n",
    "\n",
    "        elif len(data) == 1 and data == [3]:\n",
    "            obj.to_sql('NONAOE2MATCH_FAILS',con=engine, if_exists=\"append\",index=False)\n",
    "            fails.append(match_id)\n",
    "\n",
    "        else:\n",
    "            data.to_sql('RAW_MATCH_DATA',con=engine, if_exists=\"append\",index=False)\n",
    "    except:\n",
    "        obj.to_sql('UNKNOWN_FAILS',con=engine, if_exists=\"append\",index=False)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to iterate over multiple matches parallely\n",
    "def fetch_matches_iterate(start,collect,cores):\n",
    "    \n",
    "    def fetch(match_id):\n",
    "        \n",
    "        instance = voobly_login(os.environ['voobly_username'],os.environ['voobly_password'])\n",
    "        db_conn, engine = setup_sql_conn()\n",
    "        fetch_matches_write(match_id,instance,engine)        \n",
    "\n",
    "    matches = range(start,start+collect) \n",
    "    \n",
    "    # Parallel fetch matches\n",
    "    inputs = tqdm(matches,miniters = 1,bar_format='{desc:<5.5}{percentage:3.0f}%|{bar:60}{r_bar}')\n",
    "    \n",
    "    if __name__ == \"__main__\":\n",
    "        processed_list = Parallel(n_jobs=cores)(delayed(fetch)(i) for i in inputs)\n",
    "                \n",
    "    print('\\nDONE !')\n",
    "    print('\\nAOE2 MATCHES DOWNLOADED : ' + str(collect))    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc-autonumbering": false,
  "toc-showcode": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
